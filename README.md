# ğŸ–¼ï¸â¡ï¸ğŸ‡§ğŸ‡© Image-to-Bangla Caption Generation (ViT + Transformer)

This project focuses on generating **Bangla natural language captions from images** using a modern **Visionâ€“Language Encoderâ€“Decoder architecture**.

Most image captioning research is heavily **English-centric**, while Bangla (spoken by **230M+** people) has limited high-quality visionâ€“language resources. This work aims to improve Bangla caption generation by using stronger visual representation learning and better cross-modal alignment. îˆ€fileciteîˆ‚turn1file0îˆ

---

## âœ¨ Key Highlights

- âœ… Bangla Image Captioning (Vision â†’ Language)
- âœ… **Vision Transformer (ViT)** image encoder
- âœ… **Transformer** text decoder
- âœ… **Gated Cross-Attention Fusion (CAF)** for better visionâ€“language alignment
- âœ… Trained and tested on a Bangla caption dataset (BNLIT)

---

## ğŸ§  Model Architecture

### **Encoder**
- **Vision Transformer (ViT)**
- Splits an image into patches â†’ converts them into tokens
- Self-attention learns **global context from early layers** îˆ€fileciteîˆ‚turn1file0îˆ

### **Fusion Module**
- **Gated Cross-Attention Fusion (CAF)**
- Refines visual tokens and balances raw vs contextualized features îˆ€fileciteîˆ‚turn1file0îˆ

### **Decoder**
- **Transformer Decoder**
- Uses cross-attention over visual tokens to generate Bangla caption tokens îˆ€fileciteîˆ‚turn1file0îˆ

---

## ğŸ“Œ Dataset Used

This project uses the **Bangla Natural Language Image to Text (BNLIT)** dataset:

- **8,743 images**
- **Bangladesh perspective images**
- **1 Bangla caption per image**
- Preprocessed image sizes:
  - **224Ã—224**
  - **500Ã—375**
- Includes a CNN feature file: `features.pkl`

ğŸ”— Dataset link (Mendeley Data):  
https://data.mendeley.com/datasets/ws3r82gnm8/4

---

## ğŸ“‚ Project Files

Main notebook in this project:

- `ViT.ipynb` *(training + evaluation + caption generation)*

---

## âš™ï¸ Requirements

Recommended Python version:
- **Python 3.8+**

Install common dependencies:

```bash
pip install numpy pandas matplotlib pillow tqdm scikit-learn torch torchvision transformers
```

*(If your notebook uses TensorFlow/Keras instead, install those libraries accordingly.)*

---

## â–¶ï¸ How to Run

### âœ… Option 1: Run Notebook

1. Clone the repository:
   ```bash
   git clone <your-repo-link>
   cd <your-repo-folder>
   ```

2. Download the dataset from Mendeley and place it in your project directory.

3. Open the notebook:
   ```bash
   jupyter notebook ViT.ipynb
   ```

4. Run all cells to:
   - load images + captions  
   - preprocess tokens  
   - train the model  
   - generate Bangla captions  

---

## ğŸ“Š Motivation

Bangla captioning is still challenging because:

- Most datasets/models are optimized for English
- Bangla datasets are smaller and often limited in caption diversity
- CNN-based encoders may lose global context by collapsing features into a single vector îˆ€fileciteîˆ‚turn1file0îˆ

This project addresses these gaps using a **ViT-based encoder** and improved attention fusion.

---

## âœ… Expected Outcome

- Better global image understanding using ViT tokens  
- Improved alignment between **image regions â†” Bangla words**
- More natural and context-aware Bangla captions îˆ€fileciteîˆ‚turn1file0îˆ

---

## ğŸ‘¨â€ğŸ’» Author

**Hasin Almas Sifat**  
ğŸ“§ Email: hasin.almas.sifat@gmail.com  
ğŸ”— LinkedIn: https://www.linkedin.com/in/hasin-almas-sifat/  
ğŸ’» GitHub: https://github.com/almassifat  

---

## â­ Support

If you find this project helpful:
- â­ Star the repo  
- ğŸ´ Fork it  
- ğŸ§  Share feedback / improvements  
